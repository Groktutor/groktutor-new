{"ast":null,"code":"// dataService.js - Handles all external API integrations and live data fetching\nimport axios from 'axios';\n\n// Configuration for API endpoints and authentication\nconst API_CONFIG = {\n  reddit: {\n    baseUrl: 'https://oauth.reddit.com',\n    subreddits: ['PromptEngineering', 'MachineLearning', 'artificial', 'LocalLLaMA', 'OpenAI', 'AnthropicClaude'],\n    clientId: process.env.REACT_APP_REDDIT_CLIENT_ID,\n    clientSecret: process.env.REACT_APP_REDDIT_CLIENT_SECRET\n  },\n  twitter: {\n    baseUrl: 'https://api.twitter.com/2',\n    bearerToken: process.env.REACT_APP_TWITTER_BEARER_TOKEN,\n    searchQueries: ['prompt engineering', 'LLM prompting', 'AI prompts', 'ChatGPT prompts', 'Claude prompts']\n  },\n  github: {\n    baseUrl: 'https://api.github.com',\n    token: process.env.REACT_APP_GITHUB_TOKEN,\n    topics: ['prompt-engineering', 'llm', 'ai-prompts', 'langchain', 'llama-index']\n  },\n  huggingface: {\n    baseUrl: 'https://huggingface.co/api',\n    token: process.env.REACT_APP_HUGGINGFACE_TOKEN\n  }\n};\n\n// Sample data for development (used when API keys are not available)\nconst SAMPLE_DATA = {\n  reddit: [{\n    id: 'rd1',\n    title: 'How to use chain-of-thought with GPT-4 Turbo',\n    subreddit: 'PromptEngineering',\n    author: 'prompt_master',\n    score: 524,\n    num_comments: 78,\n    created: new Date(Date.now() - 12 * 60 * 60 * 1000).toISOString(),\n    selftext: \"I've been experimenting with chain-of-thought prompting in GPT-4 Turbo and found some interesting patterns. The key is to explicitly ask the model to think step by step before giving its final answer. Here's what worked best for me:\\n\\n1. Start with a clear problem statement\\n2. Add 'Let's think through this step by step'\\n3. For math problems, add 'Let's calculate this carefully'\\n4. For logic puzzles, add 'Let's analyze each possibility'\\n\\nI've found that this approach reduces errors by about 30% on complex reasoning tasks.\"\n  }, {\n    id: 'rd2',\n    title: 'I built a tool that automatically generates semantic search prompts',\n    subreddit: 'MachineLearning',\n    author: 'ai_dev42',\n    score: 412,\n    num_comments: 53,\n    created: new Date(Date.now() - 15 * 60 * 60 * 1000).toISOString(),\n    selftext: \"After months of fine-tuning, I've created a system that can generate optimal embedding prompts for semantic search. Here's how it works:\\n\\n- The system analyzes your document corpus to identify key terminology and concepts\\n- It then generates multiple candidate prompts for embedding generation\\n- Each candidate is evaluated against a validation set of known relevant pairs\\n- The best performing prompt is selected and used for production\\n\\nIn our benchmarks, this approach improved semantic search accuracy by 18% compared to standard prompts.\"\n  }, {\n    id: 'rd3',\n    title: 'Prompt template library for specialized medical queries',\n    subreddit: 'artificial',\n    author: 'health_ai_researcher',\n    score: 367,\n    num_comments: 42,\n    created: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),\n    selftext: \"I've compiled a library of 50+ prompt templates specifically designed for medical research and diagnostic assistance. Each template has been validated by healthcare professionals and optimized for different medical specialties. The templates include:\\n\\n- Differential diagnosis frameworks\\n- Medical literature search optimizers\\n- Patient symptom analyzers\\n- Treatment plan generators\\n- Medical image description enhancers\\n\\nAll templates are available on GitHub under an MIT license. They're designed to be used with major LLMs including Claude, GPT-4, and Llama 3.\"\n  }, {\n    id: 'rd4',\n    title: 'The effectiveness of Tree-of-Thought vs Chain-of-Thought',\n    subreddit: 'LocalLLaMA',\n    author: 'reasoning_researcher',\n    score: 289,\n    num_comments: 34,\n    created: new Date(Date.now() - 10 * 60 * 60 * 1000).toISOString(),\n    selftext: \"I ran a comprehensive benchmark comparing Tree-of-Thought (ToT) versus Chain-of-Thought (CoT) prompting across several open-source models. Here's what I found:\\n\\n- For simple reasoning problems, CoT is sufficient and more efficient\\n- For complex problems with multiple possible approaches, ToT significantly outperforms CoT\\n- The performance gap increases with model size (larger models benefit more from ToT)\\n- ToT requires approximately 3x more tokens, but delivers up to 2x accuracy on complex problems\\n\\nInterestingly, for models below 13B parameters, the benefits of ToT diminish substantially.\"\n  }, {\n    id: 'rd5',\n    title: 'Persona-based prompting improved my creative writing results',\n    subreddit: 'PromptEngineering',\n    author: 'novel_writer',\n    score: 311,\n    num_comments: 47,\n    created: new Date(Date.now() - 18 * 60 * 60 * 1000).toISOString(),\n    selftext: \"I've been struggling to get high-quality creative writing from LLMs until I discovered persona-based prompting. By explicitly defining the 'character' of the AI, I've seen dramatic improvements. Here's my approach:\\n\\n1. Create a detailed persona (e.g., 'You are an award-winning science fiction author known for vivid world-building and complex characters')\\n2. Define the persona's writing style specifically ('Your prose is lyrical but concise, with an emphasis on sensory details')\\n3. Provide example passages that match the desired style\\n4. Specify the output format and length\\n\\nThe difference in quality is remarkable - the writing has a consistent voice and style throughout.\"\n  }],\n  twitter: [{\n    id: 'tw1',\n    content: \"Just published my research on using multi-shot prompting for cross-lingual knowledge transfer. Results show 43% improvement over zero-shot baselines across 12 languages. #PromptEngineering #NLP\",\n    username: \"@ai_researcher\",\n    displayName: \"Dr. Sarah Chen | AI Research\",\n    likes: 284,\n    retweets: 92,\n    created: new Date(Date.now() - 8 * 60 * 60 * 1000).toISOString(),\n    url: \"https://twitter.com/ai_researcher/status/1\"\n  }, {\n    id: 'tw2',\n    content: \"The secret to good prompting isn't complexity, it's clarity. I've had better results with simple, direct prompts that clearly define the task, context, and desired output format. Don't overthink it! #LLM #GPT4\",\n    username: \"@prompt_engineer\",\n    displayName: \"Mark Johnson\",\n    likes: 342,\n    retweets: 118,\n    created: new Date(Date.now() - 5 * 60 * 60 * 1000).toISOString(),\n    url: \"https://twitter.com/prompt_engineer/status/1\"\n  }, {\n    id: 'tw3',\n    content: \"ðŸ§µ 1/5 Let's talk about the ReAct framework and how it's changing agent design. ReAct combines reasoning and action - allowing LLMs to generate both thoughts AND actions in an interleaved manner. This enables complex planning and tool use...\",\n    username: \"@llm_agents\",\n    displayName: \"Alex | Building AI Agents\",\n    likes: 512,\n    retweets: 203,\n    created: new Date(Date.now() - 11 * 60 * 60 * 1000).toISOString(),\n    url: \"https://twitter.com/llm_agents/status/1\"\n  }, {\n    id: 'tw4',\n    content: \"New blog post: 'Advanced Prompt Engineering Patterns for Claude 3' - I cover techniques specifically optimized for Claude's reasoning capabilities, including structured XML outputs, multi-persona debates, and specialized knowledge elicitation. Link in bio!\",\n    username: \"@claude_expert\",\n    displayName: \"Claude Techniques\",\n    likes: 276,\n    retweets: 87,\n    created: new Date(Date.now() - 14 * 60 * 60 * 1000).toISOString(),\n    url: \"https://twitter.com/claude_expert/status/1\"\n  }, {\n    id: 'tw5',\n    content: \"We're seeing a 35% reduction in hallucinations by using a simple prompt prefix that enforces citation of sources and explicit uncertainty marking. Will share the full technique at next week's #PromptEngineering conference.\",\n    username: \"@ai_safety_lab\",\n    displayName: \"AI Safety Research Lab\",\n    likes: 428,\n    retweets: 156,\n    created: new Date(Date.now() - 10 * 60 * 60 * 1000).toISOString(),\n    url: \"https://twitter.com/ai_safety_lab/status/1\"\n  }],\n  github: [{\n    id: 'gh1',\n    repo: 'prompt-engineering-guide',\n    author: 'dair-ai',\n    stars: 35420,\n    forks: 3180,\n    description: 'A comprehensive guide on prompt engineering for large language models, including techniques, examples and resources.',\n    updated: new Date(Date.now() - 3 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://github.com/dair-ai/prompt-engineering-guide',\n    language: 'Python',\n    issues: 42\n  }, {\n    id: 'gh2',\n    repo: 'langchain',\n    author: 'langchain-ai',\n    stars: 72450,\n    forks: 10780,\n    description: 'Building applications with LLMs through composability',\n    updated: new Date(Date.now() - 1 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://github.com/langchain-ai/langchain',\n    language: 'Python',\n    issues: 156\n  }, {\n    id: 'gh3',\n    repo: 'react-patterns',\n    author: 'kevinzg',\n    stars: 8320,\n    forks: 1240,\n    description: 'Implementation of the ReAct (Reasoning + Acting) framework for LLM agents with examples and best practices',\n    updated: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://github.com/kevinzg/react-patterns',\n    language: 'TypeScript',\n    issues: 28\n  }, {\n    id: 'gh4',\n    repo: 'awesome-prompt-engineering',\n    author: 'promptslab',\n    stars: 21340,\n    forks: 2150,\n    description: 'A curated list of awesome prompt engineering tools, papers, tutorials, and resources',\n    updated: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://github.com/promptslab/awesome-prompt-engineering',\n    language: 'Markdown',\n    issues: 15\n  }, {\n    id: 'gh5',\n    repo: 'tree-of-thought-llm',\n    author: 'kyegomez',\n    stars: 5830,\n    forks: 780,\n    description: 'Implementation of Tree of Thought (ToT) prompting for enhanced problem-solving in LLMs',\n    updated: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://github.com/kyegomez/tree-of-thought-llm',\n    language: 'Python',\n    issues: 23\n  }],\n  huggingface: [{\n    id: 'hf1',\n    model: 'prompt-tuning-toolkit',\n    author: 'promptengineering',\n    downloads: 12450,\n    description: 'A comprehensive toolkit for prompt tuning with various LLMs',\n    updated: new Date(Date.now() - 10 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://huggingface.co/promptengineering/prompt-tuning-toolkit',\n    tags: ['prompt-tuning', 'llm', 'optimization']\n  }, {\n    id: 'hf2',\n    model: 'react-prompting-model',\n    author: 'ai-research',\n    downloads: 9875,\n    description: 'Implementation of ReAct framework for enhanced reasoning capabilities',\n    updated: new Date(Date.now() - 15 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://huggingface.co/ai-research/react-prompting-model',\n    tags: ['react', 'reasoning', 'llm']\n  }, {\n    id: 'hf3',\n    model: 'chain-of-thought-llama',\n    author: 'llama-labs',\n    downloads: 8732,\n    description: 'Fine-tuned Llama model for chain-of-thought prompting',\n    updated: new Date(Date.now() - 20 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://huggingface.co/llama-labs/chain-of-thought-llama',\n    tags: ['chain-of-thought', 'llama', 'reasoning']\n  }, {\n    id: 'hf4',\n    model: 'prompt-engineering-dataset',\n    author: 'openai-community',\n    downloads: 15280,\n    description: 'Large dataset of optimized prompts for various tasks and domains',\n    updated: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://huggingface.co/datasets/openai-community/prompt-engineering-dataset',\n    tags: ['dataset', 'prompt-engineering', 'examples']\n  }, {\n    id: 'hf5',\n    model: 'medical-prompt-tuning',\n    author: 'healthcare-ai',\n    downloads: 7340,\n    description: 'Specialized prompt tuning models for healthcare and medical applications',\n    updated: new Date(Date.now() - 12 * 24 * 60 * 60 * 1000).toISOString(),\n    url: 'https://huggingface.co/healthcare-ai/medical-prompt-tuning',\n    tags: ['healthcare', 'medical', 'prompt-tuning']\n  }]\n};\n\n/**\r\n * Reddit Data Service\r\n * Fetches trending posts from prompt engineering related subreddits\r\n */\nexport const fetchRedditData = async (limit = 10) => {\n  try {\n    // Check if we're in development mode or missing API keys\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.reddit.clientId) {\n      console.log('Using sample Reddit data');\n      return SAMPLE_DATA.reddit.slice(0, limit);\n    }\n\n    // In a production environment, we would implement the actual Reddit API calls\n    // First, get an OAuth token\n    const tokenResponse = await axios.post('https://www.reddit.com/api/v1/access_token', 'grant_type=client_credentials', {\n      headers: {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Authorization': `Basic ${btoa(`${API_CONFIG.reddit.clientId}:${API_CONFIG.reddit.clientSecret}`)}`\n      }\n    });\n    const accessToken = tokenResponse.data.access_token;\n\n    // Prepare subreddit queries\n    const subredditQueries = API_CONFIG.reddit.subreddits.map(sub => axios.get(`${API_CONFIG.reddit.baseUrl}/r/${sub}/hot?limit=${Math.ceil(limit / API_CONFIG.reddit.subreddits.length)}`, {\n      headers: {\n        'Authorization': `Bearer ${accessToken}`,\n        'User-Agent': 'Groktutor/1.0.0'\n      }\n    }));\n\n    // Execute all queries in parallel\n    const responses = await Promise.all(subredditQueries);\n\n    // Process and normalize the data\n    let allPosts = [];\n    responses.forEach(response => {\n      if (response.data.data && response.data.data.children) {\n        const subredditPosts = response.data.data.children.map(child => ({\n          id: child.data.id,\n          title: child.data.title,\n          score: child.data.score,\n          author: child.data.author,\n          subreddit: `r/${child.data.subreddit}`,\n          created: new Date(child.data.created_utc * 1000).toISOString(),\n          url: `https://reddit.com${child.data.permalink}`,\n          num_comments: child.data.num_comments,\n          selftext: child.data.selftext\n        }));\n        allPosts = [...allPosts, ...subredditPosts];\n      }\n    });\n\n    // Sort by score and limit to the requested number\n    return allPosts.sort((a, b) => b.score - a.score).slice(0, limit);\n  } catch (error) {\n    console.error('Error fetching Reddit data:', error);\n    // Fallback to sample data in case of error\n    return SAMPLE_DATA.reddit.slice(0, limit);\n  }\n};\n\n/**\r\n * Twitter/X Data Service\r\n * Fetches tweets related to prompt engineering\r\n */\nexport const fetchTwitterData = async (limit = 10) => {\n  try {\n    // Check if we're in development mode or missing API keys\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.twitter.bearerToken) {\n      console.log('Using sample Twitter data');\n      return SAMPLE_DATA.twitter.slice(0, limit);\n    }\n\n    // In a production environment, we would implement the actual Twitter API calls\n    // Prepare search queries with recent popular tweets\n    const searchQueries = API_CONFIG.twitter.searchQueries.map(query => axios.get(`${API_CONFIG.twitter.baseUrl}/tweets/search/recent?query=${encodeURIComponent(query)}&max_results=${Math.ceil(limit / API_CONFIG.twitter.searchQueries.length)}&tweet.fields=public_metrics,created_at&expansions=author_id&user.fields=name,username`, {\n      headers: {\n        'Authorization': `Bearer ${API_CONFIG.twitter.bearerToken}`\n      }\n    }));\n\n    // Execute all queries in parallel\n    const responses = await Promise.all(searchQueries);\n\n    // Process and normalize the data\n    let allTweets = [];\n    responses.forEach(response => {\n      if (response.data && response.data.data) {\n        // Create a map of user data\n        const users = {};\n        if (response.data.includes && response.data.includes.users) {\n          response.data.includes.users.forEach(user => {\n            users[user.id] = {\n              username: user.username,\n              displayName: user.name\n            };\n          });\n        }\n        const tweets = response.data.data.map(tweet => {\n          var _users$tweet$author_i;\n          return {\n            id: tweet.id,\n            content: tweet.text,\n            username: users[tweet.author_id] ? `@${users[tweet.author_id].username}` : '@unknown',\n            displayName: users[tweet.author_id] ? users[tweet.author_id].displayName : 'Unknown User',\n            likes: tweet.public_metrics.like_count,\n            retweets: tweet.public_metrics.retweet_count,\n            created: tweet.created_at,\n            url: `https://twitter.com/${(_users$tweet$author_i = users[tweet.author_id]) === null || _users$tweet$author_i === void 0 ? void 0 : _users$tweet$author_i.username}/status/${tweet.id}`\n          };\n        });\n        allTweets = [...allTweets, ...tweets];\n      }\n    });\n\n    // Sort by engagement (likes + retweets) and limit to the requested number\n    return allTweets.sort((a, b) => b.likes + b.retweets - (a.likes + a.retweets)).slice(0, limit);\n  } catch (error) {\n    console.error('Error fetching Twitter data:', error);\n    // Fallback to sample data in case of error\n    return SAMPLE_DATA.twitter.slice(0, limit);\n  }\n};\n\n/**\r\n * GitHub Data Service\r\n * Fetches trending repositories related to prompt engineering\r\n */\nexport const fetchGithubData = async (limit = 10) => {\n  try {\n    // Check if we're in development mode or missing API keys\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.github.token) {\n      console.log('Using sample GitHub data');\n      return SAMPLE_DATA.github.slice(0, limit);\n    }\n\n    // In a production environment, we would implement the actual GitHub API calls\n    // Prepare topic queries to find repositories\n    const topicQueries = API_CONFIG.github.topics.map(topic => axios.get(`${API_CONFIG.github.baseUrl}/search/repositories?q=topic:${topic}&sort=stars&order=desc&per_page=${Math.ceil(limit / API_CONFIG.github.topics.length)}`, {\n      headers: {\n        'Authorization': `token ${API_CONFIG.github.token}`,\n        'Accept': 'application/vnd.github.v3+json'\n      }\n    }));\n\n    // Execute all queries in parallel\n    const responses = await Promise.all(topicQueries);\n\n    // Process and normalize the data\n    let allRepos = [];\n    responses.forEach(response => {\n      if (response.data && response.data.items) {\n        const repos = response.data.items.map(item => ({\n          id: item.id.toString(),\n          repo: item.name,\n          author: item.owner.login,\n          stars: item.stargazers_count,\n          description: item.description || '',\n          updated: item.updated_at,\n          url: item.html_url,\n          language: item.language,\n          forks: item.forks_count,\n          issues: item.open_issues_count\n        }));\n        allRepos = [...allRepos, ...repos];\n      }\n    });\n\n    // Remove duplicates (same repo might be returned for different topics)\n    const uniqueRepos = allRepos.reduce((acc, current) => {\n      const x = acc.find(item => item.id === current.id);\n      if (!x) {\n        return acc.concat([current]);\n      } else {\n        return acc;\n      }\n    }, []);\n\n    // Sort by stars and limit to the requested number\n    return uniqueRepos.sort((a, b) => b.stars - a.stars).slice(0, limit);\n  } catch (error) {\n    console.error('Error fetching GitHub data:', error);\n    // Fallback to sample data in case of error\n    return SAMPLE_DATA.github.slice(0, limit);\n  }\n};\n\n/**\r\n * HuggingFace Data Service\r\n * Fetches popular models related to prompt engineering\r\n */\nexport const fetchHuggingfaceData = async (limit = 10) => {\n  try {\n    // Check if we're in development mode or missing API keys\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.huggingface.token) {\n      console.log('Using sample HuggingFace data');\n      return SAMPLE_DATA.huggingface.slice(0, limit);\n    }\n\n    // In a production environment, we would implement the actual HuggingFace API calls\n    // Query for models related to prompt engineering\n    const response = await axios.get(`${API_CONFIG.huggingface.baseUrl}/models?search=prompt&sort=downloads&direction=-1&limit=${limit}`, {\n      headers: {\n        'Authorization': `Bearer ${API_CONFIG.huggingface.token}`\n      }\n    });\n\n    // Process and normalize the data\n    const models = response.data.map(model => ({\n      id: model.id,\n      model: model.id.split('/').pop(),\n      author: model.id.split('/')[0],\n      downloads: model.downloads || 0,\n      description: model.description || 'No description available',\n      updated: model.lastModified,\n      url: `https://huggingface.co/${model.id}`,\n      tags: model.tags || []\n    }));\n    return models;\n  } catch (error) {\n    console.error('Error fetching HuggingFace data:', error);\n    // Fallback to sample data in case of error\n    return SAMPLE_DATA.huggingface.slice(0, limit);\n  }\n};\n\n/**\r\n * Combined data service that fetches from all sources\r\n */\nexport const fetchAllData = async () => {\n  try {\n    const [redditData, twitterData, githubData, huggingfaceData] = await Promise.all([fetchRedditData(), fetchTwitterData(), fetchGithubData(), fetchHuggingfaceData()]);\n    return {\n      reddit: redditData,\n      twitter: twitterData,\n      github: githubData,\n      huggingface: huggingfaceData,\n      timestamp: new Date().toISOString()\n    };\n  } catch (error) {\n    console.error('Error fetching all data:', error);\n    return {\n      reddit: SAMPLE_DATA.reddit,\n      twitter: SAMPLE_DATA.twitter,\n      github: SAMPLE_DATA.github,\n      huggingface: SAMPLE_DATA.huggingface,\n      timestamp: new Date().toISOString(),\n      error: error.message\n    };\n  }\n};\n\n/**\r\n * Cache service to store fetched data and reduce API calls\r\n */\nclass DataCache {\n  constructor(cacheDuration = 300000) {\n    // Default 5 minutes\n    this.cache = {};\n    this.cacheDuration = cacheDuration;\n  }\n  get(key) {\n    const cachedData = this.cache[key];\n    if (cachedData && Date.now() - cachedData.timestamp < this.cacheDuration) {\n      return cachedData.data;\n    }\n    return null;\n  }\n  set(key, data) {\n    this.cache[key] = {\n      data,\n      timestamp: Date.now()\n    };\n  }\n  invalidate(key) {\n    delete this.cache[key];\n  }\n  invalidateAll() {\n    this.cache = {};\n  }\n}\n\n// Export a singleton instance of the cache\nexport const dataCache = new DataCache();\n\n/**\r\n * Main API service with caching for the Groktutor platform\r\n */\nexport const fetchDataWithCache = async (source, forceRefresh = false) => {\n  // Define the data fetching functions for each source\n  const fetchFunctions = {\n    reddit: fetchRedditData,\n    twitter: fetchTwitterData,\n    github: fetchGithubData,\n    huggingface: fetchHuggingfaceData,\n    all: fetchAllData\n  };\n\n  // If force refresh is not required, try to get from cache first\n  if (!forceRefresh) {\n    const cachedData = dataCache.get(source);\n    if (cachedData) {\n      return cachedData;\n    }\n  }\n\n  // If not in cache or force refresh, fetch new data\n  try {\n    const fetchFunction = fetchFunctions[source];\n    if (!fetchFunction) {\n      throw new Error(`Invalid data source: ${source}`);\n    }\n    const data = await fetchFunction();\n\n    // Cache the new data\n    dataCache.set(source, data);\n    return data;\n  } catch (error) {\n    console.error(`Error fetching ${source} data:`, error);\n    throw error;\n  }\n};\n\n/**\r\n * Real-time data service for live updates\r\n */\nclass RealTimeDataService {\n  constructor() {\n    this.subscribers = {};\n    this.intervalIds = {};\n    this.isRunning = false;\n  }\n  start() {\n    if (this.isRunning) return;\n    this.isRunning = true;\n\n    // Set up periodic fetching for each source\n    this.intervalIds.reddit = setInterval(async () => {\n      try {\n        const data = await fetchDataWithCache('reddit', true);\n        this.notifySubscribers('reddit', data);\n      } catch (error) {\n        console.error('Error in real-time Reddit data:', error);\n      }\n    }, 300000); // 5 minutes\n\n    this.intervalIds.twitter = setInterval(async () => {\n      try {\n        const data = await fetchDataWithCache('twitter', true);\n        this.notifySubscribers('twitter', data);\n      } catch (error) {\n        console.error('Error in real-time Twitter data:', error);\n      }\n    }, 300000); // 5 minutes\n\n    this.intervalIds.github = setInterval(async () => {\n      try {\n        const data = await fetchDataWithCache('github', true);\n        this.notifySubscribers('github', data);\n      } catch (error) {\n        console.error('Error in real-time GitHub data:', error);\n      }\n    }, 1800000); // 30 minutes\n\n    this.intervalIds.huggingface = setInterval(async () => {\n      try {\n        const data = await fetchDataWithCache('huggingface', true);\n        this.notifySubscribers('huggingface', data);\n      } catch (error) {\n        console.error('Error in real-time HuggingFace data:', error);\n      }\n    }, 3600000); // 60 minutes\n  }\n  stop() {\n    if (!this.isRunning) return;\n\n    // Clear all intervals\n    Object.values(this.intervalIds).forEach(intervalId => {\n      clearInterval(intervalId);\n    });\n    this.intervalIds = {};\n    this.isRunning = false;\n  }\n  subscribe(source, callback) {\n    if (!this.subscribers[source]) {\n      this.subscribers[source] = [];\n    }\n    this.subscribers[source].push(callback);\n\n    // Return an unsubscribe function\n    return () => {\n      this.subscribers[source] = this.subscribers[source].filter(cb => cb !== callback);\n    };\n  }\n  notifySubscribers(source, data) {\n    if (this.subscribers[source]) {\n      this.subscribers[source].forEach(callback => {\n        callback(data);\n      });\n    }\n  }\n}\n\n// Export a singleton instance of the real-time service\nexport const realTimeService = new RealTimeDataService();\n\n// Initialize the real-time service on startup\nrealTimeService.start();","map":{"version":3,"names":["axios","API_CONFIG","reddit","baseUrl","subreddits","clientId","process","env","REACT_APP_REDDIT_CLIENT_ID","clientSecret","REACT_APP_REDDIT_CLIENT_SECRET","twitter","bearerToken","REACT_APP_TWITTER_BEARER_TOKEN","searchQueries","github","token","REACT_APP_GITHUB_TOKEN","topics","huggingface","REACT_APP_HUGGINGFACE_TOKEN","SAMPLE_DATA","id","title","subreddit","author","score","num_comments","created","Date","now","toISOString","selftext","content","username","displayName","likes","retweets","url","repo","stars","forks","description","updated","language","issues","model","downloads","tags","fetchRedditData","limit","NODE_ENV","console","log","slice","tokenResponse","post","headers","btoa","accessToken","data","access_token","subredditQueries","map","sub","get","Math","ceil","length","responses","Promise","all","allPosts","forEach","response","children","subredditPosts","child","created_utc","permalink","sort","a","b","error","fetchTwitterData","query","encodeURIComponent","allTweets","users","includes","user","name","tweets","tweet","_users$tweet$author_i","text","author_id","public_metrics","like_count","retweet_count","created_at","fetchGithubData","topicQueries","topic","allRepos","items","repos","item","toString","owner","login","stargazers_count","updated_at","html_url","forks_count","open_issues_count","uniqueRepos","reduce","acc","current","x","find","concat","fetchHuggingfaceData","models","split","pop","lastModified","fetchAllData","redditData","twitterData","githubData","huggingfaceData","timestamp","message","DataCache","constructor","cacheDuration","cache","key","cachedData","set","invalidate","invalidateAll","dataCache","fetchDataWithCache","source","forceRefresh","fetchFunctions","fetchFunction","Error","RealTimeDataService","subscribers","intervalIds","isRunning","start","setInterval","notifySubscribers","stop","Object","values","intervalId","clearInterval","subscribe","callback","push","filter","cb","realTimeService"],"sources":["D:/Groktutuor/groktutor-new/src/services/dataService.js"],"sourcesContent":["// dataService.js - Handles all external API integrations and live data fetching\r\nimport axios from 'axios';\r\n\r\n// Configuration for API endpoints and authentication\r\nconst API_CONFIG = {\r\n  reddit: {\r\n    baseUrl: 'https://oauth.reddit.com',\r\n    subreddits: ['PromptEngineering', 'MachineLearning', 'artificial', 'LocalLLaMA', 'OpenAI', 'AnthropicClaude'],\r\n    clientId: process.env.REACT_APP_REDDIT_CLIENT_ID,\r\n    clientSecret: process.env.REACT_APP_REDDIT_CLIENT_SECRET\r\n  },\r\n  twitter: {\r\n    baseUrl: 'https://api.twitter.com/2',\r\n    bearerToken: process.env.REACT_APP_TWITTER_BEARER_TOKEN,\r\n    searchQueries: ['prompt engineering', 'LLM prompting', 'AI prompts', 'ChatGPT prompts', 'Claude prompts']\r\n  },\r\n  github: {\r\n    baseUrl: 'https://api.github.com',\r\n    token: process.env.REACT_APP_GITHUB_TOKEN,\r\n    topics: ['prompt-engineering', 'llm', 'ai-prompts', 'langchain', 'llama-index']\r\n  },\r\n  huggingface: {\r\n    baseUrl: 'https://huggingface.co/api',\r\n    token: process.env.REACT_APP_HUGGINGFACE_TOKEN\r\n  }\r\n};\r\n\r\n// Sample data for development (used when API keys are not available)\r\nconst SAMPLE_DATA = {\r\n  reddit: [\r\n    {\r\n      id: 'rd1',\r\n      title: 'How to use chain-of-thought with GPT-4 Turbo',\r\n      subreddit: 'PromptEngineering',\r\n      author: 'prompt_master',\r\n      score: 524,\r\n      num_comments: 78,\r\n      created: new Date(Date.now() - 12 * 60 * 60 * 1000).toISOString(),\r\n      selftext: \"I've been experimenting with chain-of-thought prompting in GPT-4 Turbo and found some interesting patterns. The key is to explicitly ask the model to think step by step before giving its final answer. Here's what worked best for me:\\n\\n1. Start with a clear problem statement\\n2. Add 'Let's think through this step by step'\\n3. For math problems, add 'Let's calculate this carefully'\\n4. For logic puzzles, add 'Let's analyze each possibility'\\n\\nI've found that this approach reduces errors by about 30% on complex reasoning tasks.\"\r\n    },\r\n    {\r\n      id: 'rd2',\r\n      title: 'I built a tool that automatically generates semantic search prompts',\r\n      subreddit: 'MachineLearning',\r\n      author: 'ai_dev42',\r\n      score: 412,\r\n      num_comments: 53,\r\n      created: new Date(Date.now() - 15 * 60 * 60 * 1000).toISOString(),\r\n      selftext: \"After months of fine-tuning, I've created a system that can generate optimal embedding prompts for semantic search. Here's how it works:\\n\\n- The system analyzes your document corpus to identify key terminology and concepts\\n- It then generates multiple candidate prompts for embedding generation\\n- Each candidate is evaluated against a validation set of known relevant pairs\\n- The best performing prompt is selected and used for production\\n\\nIn our benchmarks, this approach improved semantic search accuracy by 18% compared to standard prompts.\"\r\n    },\r\n    {\r\n      id: 'rd3',\r\n      title: 'Prompt template library for specialized medical queries',\r\n      subreddit: 'artificial',\r\n      author: 'health_ai_researcher',\r\n      score: 367,\r\n      num_comments: 42,\r\n      created: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),\r\n      selftext: \"I've compiled a library of 50+ prompt templates specifically designed for medical research and diagnostic assistance. Each template has been validated by healthcare professionals and optimized for different medical specialties. The templates include:\\n\\n- Differential diagnosis frameworks\\n- Medical literature search optimizers\\n- Patient symptom analyzers\\n- Treatment plan generators\\n- Medical image description enhancers\\n\\nAll templates are available on GitHub under an MIT license. They're designed to be used with major LLMs including Claude, GPT-4, and Llama 3.\"\r\n    },\r\n    {\r\n      id: 'rd4',\r\n      title: 'The effectiveness of Tree-of-Thought vs Chain-of-Thought',\r\n      subreddit: 'LocalLLaMA',\r\n      author: 'reasoning_researcher',\r\n      score: 289,\r\n      num_comments: 34,\r\n      created: new Date(Date.now() - 10 * 60 * 60 * 1000).toISOString(),\r\n      selftext: \"I ran a comprehensive benchmark comparing Tree-of-Thought (ToT) versus Chain-of-Thought (CoT) prompting across several open-source models. Here's what I found:\\n\\n- For simple reasoning problems, CoT is sufficient and more efficient\\n- For complex problems with multiple possible approaches, ToT significantly outperforms CoT\\n- The performance gap increases with model size (larger models benefit more from ToT)\\n- ToT requires approximately 3x more tokens, but delivers up to 2x accuracy on complex problems\\n\\nInterestingly, for models below 13B parameters, the benefits of ToT diminish substantially.\"\r\n    },\r\n    {\r\n      id: 'rd5',\r\n      title: 'Persona-based prompting improved my creative writing results',\r\n      subreddit: 'PromptEngineering',\r\n      author: 'novel_writer',\r\n      score: 311,\r\n      num_comments: 47,\r\n      created: new Date(Date.now() - 18 * 60 * 60 * 1000).toISOString(),\r\n      selftext: \"I've been struggling to get high-quality creative writing from LLMs until I discovered persona-based prompting. By explicitly defining the 'character' of the AI, I've seen dramatic improvements. Here's my approach:\\n\\n1. Create a detailed persona (e.g., 'You are an award-winning science fiction author known for vivid world-building and complex characters')\\n2. Define the persona's writing style specifically ('Your prose is lyrical but concise, with an emphasis on sensory details')\\n3. Provide example passages that match the desired style\\n4. Specify the output format and length\\n\\nThe difference in quality is remarkable - the writing has a consistent voice and style throughout.\"\r\n    }\r\n  ],\r\n  twitter: [\r\n    {\r\n      id: 'tw1',\r\n      content: \"Just published my research on using multi-shot prompting for cross-lingual knowledge transfer. Results show 43% improvement over zero-shot baselines across 12 languages. #PromptEngineering #NLP\",\r\n      username: \"@ai_researcher\",\r\n      displayName: \"Dr. Sarah Chen | AI Research\",\r\n      likes: 284,\r\n      retweets: 92,\r\n      created: new Date(Date.now() - 8 * 60 * 60 * 1000).toISOString(),\r\n      url: \"https://twitter.com/ai_researcher/status/1\"\r\n    },\r\n    {\r\n      id: 'tw2',\r\n      content: \"The secret to good prompting isn't complexity, it's clarity. I've had better results with simple, direct prompts that clearly define the task, context, and desired output format. Don't overthink it! #LLM #GPT4\",\r\n      username: \"@prompt_engineer\",\r\n      displayName: \"Mark Johnson\",\r\n      likes: 342,\r\n      retweets: 118,\r\n      created: new Date(Date.now() - 5 * 60 * 60 * 1000).toISOString(),\r\n      url: \"https://twitter.com/prompt_engineer/status/1\"\r\n    },\r\n    {\r\n      id: 'tw3',\r\n      content: \"ðŸ§µ 1/5 Let's talk about the ReAct framework and how it's changing agent design. ReAct combines reasoning and action - allowing LLMs to generate both thoughts AND actions in an interleaved manner. This enables complex planning and tool use...\",\r\n      username: \"@llm_agents\",\r\n      displayName: \"Alex | Building AI Agents\",\r\n      likes: 512,\r\n      retweets: 203,\r\n      created: new Date(Date.now() - 11 * 60 * 60 * 1000).toISOString(),\r\n      url: \"https://twitter.com/llm_agents/status/1\"\r\n    },\r\n    {\r\n      id: 'tw4',\r\n      content: \"New blog post: 'Advanced Prompt Engineering Patterns for Claude 3' - I cover techniques specifically optimized for Claude's reasoning capabilities, including structured XML outputs, multi-persona debates, and specialized knowledge elicitation. Link in bio!\",\r\n      username: \"@claude_expert\",\r\n      displayName: \"Claude Techniques\",\r\n      likes: 276,\r\n      retweets: 87,\r\n      created: new Date(Date.now() - 14 * 60 * 60 * 1000).toISOString(),\r\n      url: \"https://twitter.com/claude_expert/status/1\"\r\n    },\r\n    {\r\n      id: 'tw5',\r\n      content: \"We're seeing a 35% reduction in hallucinations by using a simple prompt prefix that enforces citation of sources and explicit uncertainty marking. Will share the full technique at next week's #PromptEngineering conference.\",\r\n      username: \"@ai_safety_lab\",\r\n      displayName: \"AI Safety Research Lab\",\r\n      likes: 428,\r\n      retweets: 156,\r\n      created: new Date(Date.now() - 10 * 60 * 60 * 1000).toISOString(),\r\n      url: \"https://twitter.com/ai_safety_lab/status/1\"\r\n    }\r\n  ],\r\n  github: [\r\n    {\r\n      id: 'gh1',\r\n      repo: 'prompt-engineering-guide',\r\n      author: 'dair-ai',\r\n      stars: 35420,\r\n      forks: 3180,\r\n      description: 'A comprehensive guide on prompt engineering for large language models, including techniques, examples and resources.',\r\n      updated: new Date(Date.now() - 3 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://github.com/dair-ai/prompt-engineering-guide',\r\n      language: 'Python',\r\n      issues: 42\r\n    },\r\n    {\r\n      id: 'gh2',\r\n      repo: 'langchain',\r\n      author: 'langchain-ai',\r\n      stars: 72450,\r\n      forks: 10780,\r\n      description: 'Building applications with LLMs through composability',\r\n      updated: new Date(Date.now() - 1 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://github.com/langchain-ai/langchain',\r\n      language: 'Python',\r\n      issues: 156\r\n    },\r\n    {\r\n      id: 'gh3',\r\n      repo: 'react-patterns',\r\n      author: 'kevinzg',\r\n      stars: 8320,\r\n      forks: 1240,\r\n      description: 'Implementation of the ReAct (Reasoning + Acting) framework for LLM agents with examples and best practices',\r\n      updated: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://github.com/kevinzg/react-patterns',\r\n      language: 'TypeScript',\r\n      issues: 28\r\n    },\r\n    {\r\n      id: 'gh4',\r\n      repo: 'awesome-prompt-engineering',\r\n      author: 'promptslab',\r\n      stars: 21340,\r\n      forks: 2150,\r\n      description: 'A curated list of awesome prompt engineering tools, papers, tutorials, and resources',\r\n      updated: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://github.com/promptslab/awesome-prompt-engineering',\r\n      language: 'Markdown',\r\n      issues: 15\r\n    },\r\n    {\r\n      id: 'gh5',\r\n      repo: 'tree-of-thought-llm',\r\n      author: 'kyegomez',\r\n      stars: 5830,\r\n      forks: 780,\r\n      description: 'Implementation of Tree of Thought (ToT) prompting for enhanced problem-solving in LLMs',\r\n      updated: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://github.com/kyegomez/tree-of-thought-llm',\r\n      language: 'Python',\r\n      issues: 23\r\n    }\r\n  ],\r\n  huggingface: [\r\n    {\r\n      id: 'hf1',\r\n      model: 'prompt-tuning-toolkit',\r\n      author: 'promptengineering',\r\n      downloads: 12450,\r\n      description: 'A comprehensive toolkit for prompt tuning with various LLMs',\r\n      updated: new Date(Date.now() - 10 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://huggingface.co/promptengineering/prompt-tuning-toolkit',\r\n      tags: ['prompt-tuning', 'llm', 'optimization']\r\n    },\r\n    {\r\n      id: 'hf2',\r\n      model: 'react-prompting-model',\r\n      author: 'ai-research',\r\n      downloads: 9875,\r\n      description: 'Implementation of ReAct framework for enhanced reasoning capabilities',\r\n      updated: new Date(Date.now() - 15 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://huggingface.co/ai-research/react-prompting-model',\r\n      tags: ['react', 'reasoning', 'llm']\r\n    },\r\n    {\r\n      id: 'hf3',\r\n      model: 'chain-of-thought-llama',\r\n      author: 'llama-labs',\r\n      downloads: 8732,\r\n      description: 'Fine-tuned Llama model for chain-of-thought prompting',\r\n      updated: new Date(Date.now() - 20 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://huggingface.co/llama-labs/chain-of-thought-llama',\r\n      tags: ['chain-of-thought', 'llama', 'reasoning']\r\n    },\r\n    {\r\n      id: 'hf4',\r\n      model: 'prompt-engineering-dataset',\r\n      author: 'openai-community',\r\n      downloads: 15280,\r\n      description: 'Large dataset of optimized prompts for various tasks and domains',\r\n      updated: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://huggingface.co/datasets/openai-community/prompt-engineering-dataset',\r\n      tags: ['dataset', 'prompt-engineering', 'examples']\r\n    },\r\n    {\r\n      id: 'hf5',\r\n      model: 'medical-prompt-tuning',\r\n      author: 'healthcare-ai',\r\n      downloads: 7340,\r\n      description: 'Specialized prompt tuning models for healthcare and medical applications',\r\n      updated: new Date(Date.now() - 12 * 24 * 60 * 60 * 1000).toISOString(),\r\n      url: 'https://huggingface.co/healthcare-ai/medical-prompt-tuning',\r\n      tags: ['healthcare', 'medical', 'prompt-tuning']\r\n    }\r\n  ]\r\n};\r\n\r\n/**\r\n * Reddit Data Service\r\n * Fetches trending posts from prompt engineering related subreddits\r\n */\r\nexport const fetchRedditData = async (limit = 10) => {\r\n  try {\r\n    // Check if we're in development mode or missing API keys\r\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.reddit.clientId) {\r\n      console.log('Using sample Reddit data');\r\n      return SAMPLE_DATA.reddit.slice(0, limit);\r\n    }\r\n    \r\n    // In a production environment, we would implement the actual Reddit API calls\r\n    // First, get an OAuth token\r\n    const tokenResponse = await axios.post('https://www.reddit.com/api/v1/access_token', \r\n      'grant_type=client_credentials',\r\n      {\r\n        headers: {\r\n          'Content-Type': 'application/x-www-form-urlencoded',\r\n          'Authorization': `Basic ${btoa(`${API_CONFIG.reddit.clientId}:${API_CONFIG.reddit.clientSecret}`)}`\r\n        }\r\n      }\r\n    );\r\n    \r\n    const accessToken = tokenResponse.data.access_token;\r\n    \r\n    // Prepare subreddit queries\r\n    const subredditQueries = API_CONFIG.reddit.subreddits.map(sub => \r\n      axios.get(`${API_CONFIG.reddit.baseUrl}/r/${sub}/hot?limit=${Math.ceil(limit / API_CONFIG.reddit.subreddits.length)}`, {\r\n        headers: {\r\n          'Authorization': `Bearer ${accessToken}`,\r\n          'User-Agent': 'Groktutor/1.0.0'\r\n        }\r\n      })\r\n    );\r\n    \r\n    // Execute all queries in parallel\r\n    const responses = await Promise.all(subredditQueries);\r\n    \r\n    // Process and normalize the data\r\n    let allPosts = [];\r\n    responses.forEach(response => {\r\n      if (response.data.data && response.data.data.children) {\r\n        const subredditPosts = response.data.data.children.map(child => ({\r\n          id: child.data.id,\r\n          title: child.data.title,\r\n          score: child.data.score,\r\n          author: child.data.author,\r\n          subreddit: `r/${child.data.subreddit}`,\r\n          created: new Date(child.data.created_utc * 1000).toISOString(),\r\n          url: `https://reddit.com${child.data.permalink}`,\r\n          num_comments: child.data.num_comments,\r\n          selftext: child.data.selftext\r\n        }));\r\n        allPosts = [...allPosts, ...subredditPosts];\r\n      }\r\n    });\r\n    \r\n    // Sort by score and limit to the requested number\r\n    return allPosts.sort((a, b) => b.score - a.score).slice(0, limit);\r\n  } catch (error) {\r\n    console.error('Error fetching Reddit data:', error);\r\n    // Fallback to sample data in case of error\r\n    return SAMPLE_DATA.reddit.slice(0, limit);\r\n  }\r\n};\r\n\r\n/**\r\n * Twitter/X Data Service\r\n * Fetches tweets related to prompt engineering\r\n */\r\nexport const fetchTwitterData = async (limit = 10) => {\r\n  try {\r\n    // Check if we're in development mode or missing API keys\r\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.twitter.bearerToken) {\r\n      console.log('Using sample Twitter data');\r\n      return SAMPLE_DATA.twitter.slice(0, limit);\r\n    }\r\n    \r\n    // In a production environment, we would implement the actual Twitter API calls\r\n    // Prepare search queries with recent popular tweets\r\n    const searchQueries = API_CONFIG.twitter.searchQueries.map(query => \r\n      axios.get(`${API_CONFIG.twitter.baseUrl}/tweets/search/recent?query=${encodeURIComponent(query)}&max_results=${Math.ceil(limit / API_CONFIG.twitter.searchQueries.length)}&tweet.fields=public_metrics,created_at&expansions=author_id&user.fields=name,username`, {\r\n        headers: {\r\n          'Authorization': `Bearer ${API_CONFIG.twitter.bearerToken}`\r\n        }\r\n      })\r\n    );\r\n    \r\n    // Execute all queries in parallel\r\n    const responses = await Promise.all(searchQueries);\r\n    \r\n    // Process and normalize the data\r\n    let allTweets = [];\r\n    responses.forEach(response => {\r\n      if (response.data && response.data.data) {\r\n        // Create a map of user data\r\n        const users = {};\r\n        if (response.data.includes && response.data.includes.users) {\r\n          response.data.includes.users.forEach(user => {\r\n            users[user.id] = {\r\n              username: user.username,\r\n              displayName: user.name\r\n            };\r\n          });\r\n        }\r\n        \r\n        const tweets = response.data.data.map(tweet => ({\r\n          id: tweet.id,\r\n          content: tweet.text,\r\n          username: users[tweet.author_id] ? `@${users[tweet.author_id].username}` : '@unknown',\r\n          displayName: users[tweet.author_id] ? users[tweet.author_id].displayName : 'Unknown User',\r\n          likes: tweet.public_metrics.like_count,\r\n          retweets: tweet.public_metrics.retweet_count,\r\n          created: tweet.created_at,\r\n          url: `https://twitter.com/${users[tweet.author_id]?.username}/status/${tweet.id}`\r\n        }));\r\n        \r\n        allTweets = [...allTweets, ...tweets];\r\n      }\r\n    });\r\n    \r\n    // Sort by engagement (likes + retweets) and limit to the requested number\r\n    return allTweets\r\n      .sort((a, b) => (b.likes + b.retweets) - (a.likes + a.retweets))\r\n      .slice(0, limit);\r\n  } catch (error) {\r\n    console.error('Error fetching Twitter data:', error);\r\n    // Fallback to sample data in case of error\r\n    return SAMPLE_DATA.twitter.slice(0, limit);\r\n  }\r\n};\r\n\r\n/**\r\n * GitHub Data Service\r\n * Fetches trending repositories related to prompt engineering\r\n */\r\nexport const fetchGithubData = async (limit = 10) => {\r\n  try {\r\n    // Check if we're in development mode or missing API keys\r\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.github.token) {\r\n      console.log('Using sample GitHub data');\r\n      return SAMPLE_DATA.github.slice(0, limit);\r\n    }\r\n    \r\n    // In a production environment, we would implement the actual GitHub API calls\r\n    // Prepare topic queries to find repositories\r\n    const topicQueries = API_CONFIG.github.topics.map(topic => \r\n      axios.get(`${API_CONFIG.github.baseUrl}/search/repositories?q=topic:${topic}&sort=stars&order=desc&per_page=${Math.ceil(limit / API_CONFIG.github.topics.length)}`, {\r\n        headers: {\r\n          'Authorization': `token ${API_CONFIG.github.token}`,\r\n          'Accept': 'application/vnd.github.v3+json'\r\n        }\r\n      })\r\n    );\r\n    \r\n    // Execute all queries in parallel\r\n    const responses = await Promise.all(topicQueries);\r\n    \r\n    // Process and normalize the data\r\n    let allRepos = [];\r\n    responses.forEach(response => {\r\n      if (response.data && response.data.items) {\r\n        const repos = response.data.items.map(item => ({\r\n          id: item.id.toString(),\r\n          repo: item.name,\r\n          author: item.owner.login,\r\n          stars: item.stargazers_count,\r\n          description: item.description || '',\r\n          updated: item.updated_at,\r\n          url: item.html_url,\r\n          language: item.language,\r\n          forks: item.forks_count,\r\n          issues: item.open_issues_count\r\n        }));\r\n        \r\n        allRepos = [...allRepos, ...repos];\r\n      }\r\n    });\r\n    \r\n    // Remove duplicates (same repo might be returned for different topics)\r\n    const uniqueRepos = allRepos.reduce((acc, current) => {\r\n      const x = acc.find(item => item.id === current.id);\r\n      if (!x) {\r\n        return acc.concat([current]);\r\n      } else {\r\n        return acc;\r\n      }\r\n    }, []);\r\n    \r\n    // Sort by stars and limit to the requested number\r\n    return uniqueRepos.sort((a, b) => b.stars - a.stars).slice(0, limit);\r\n  } catch (error) {\r\n    console.error('Error fetching GitHub data:', error);\r\n    // Fallback to sample data in case of error\r\n    return SAMPLE_DATA.github.slice(0, limit);\r\n  }\r\n};\r\n\r\n/**\r\n * HuggingFace Data Service\r\n * Fetches popular models related to prompt engineering\r\n */\r\nexport const fetchHuggingfaceData = async (limit = 10) => {\r\n  try {\r\n    // Check if we're in development mode or missing API keys\r\n    if (process.env.NODE_ENV === 'development' || !API_CONFIG.huggingface.token) {\r\n      console.log('Using sample HuggingFace data');\r\n      return SAMPLE_DATA.huggingface.slice(0, limit);\r\n    }\r\n    \r\n    // In a production environment, we would implement the actual HuggingFace API calls\r\n    // Query for models related to prompt engineering\r\n    const response = await axios.get(`${API_CONFIG.huggingface.baseUrl}/models?search=prompt&sort=downloads&direction=-1&limit=${limit}`, {\r\n      headers: {\r\n        'Authorization': `Bearer ${API_CONFIG.huggingface.token}`\r\n      }\r\n    });\r\n    \r\n    // Process and normalize the data\r\n    const models = response.data.map(model => ({\r\n      id: model.id,\r\n      model: model.id.split('/').pop(),\r\n      author: model.id.split('/')[0],\r\n      downloads: model.downloads || 0,\r\n      description: model.description || 'No description available',\r\n      updated: model.lastModified,\r\n      url: `https://huggingface.co/${model.id}`,\r\n      tags: model.tags || []\r\n    }));\r\n    \r\n    return models;\r\n  } catch (error) {\r\n    console.error('Error fetching HuggingFace data:', error);\r\n    // Fallback to sample data in case of error\r\n    return SAMPLE_DATA.huggingface.slice(0, limit);\r\n  }\r\n};\r\n\r\n/**\r\n * Combined data service that fetches from all sources\r\n */\r\nexport const fetchAllData = async () => {\r\n  try {\r\n    const [redditData, twitterData, githubData, huggingfaceData] = await Promise.all([\r\n      fetchRedditData(),\r\n      fetchTwitterData(),\r\n      fetchGithubData(),\r\n      fetchHuggingfaceData()\r\n    ]);\r\n    \r\n    return {\r\n      reddit: redditData,\r\n      twitter: twitterData,\r\n      github: githubData,\r\n      huggingface: huggingfaceData,\r\n      timestamp: new Date().toISOString()\r\n    };\r\n  } catch (error) {\r\n    console.error('Error fetching all data:', error);\r\n    return {\r\n      reddit: SAMPLE_DATA.reddit,\r\n      twitter: SAMPLE_DATA.twitter,\r\n      github: SAMPLE_DATA.github,\r\n      huggingface: SAMPLE_DATA.huggingface,\r\n      timestamp: new Date().toISOString(),\r\n      error: error.message\r\n    };\r\n  }\r\n};\r\n\r\n/**\r\n * Cache service to store fetched data and reduce API calls\r\n */\r\nclass DataCache {\r\n  constructor(cacheDuration = 300000) { // Default 5 minutes\r\n    this.cache = {};\r\n    this.cacheDuration = cacheDuration;\r\n  }\r\n  \r\n  get(key) {\r\n    const cachedData = this.cache[key];\r\n    if (cachedData && (Date.now() - cachedData.timestamp < this.cacheDuration)) {\r\n      return cachedData.data;\r\n    }\r\n    return null;\r\n  }\r\n  \r\n  set(key, data) {\r\n    this.cache[key] = {\r\n      data,\r\n      timestamp: Date.now()\r\n    };\r\n  }\r\n  \r\n  invalidate(key) {\r\n    delete this.cache[key];\r\n  }\r\n  \r\n  invalidateAll() {\r\n    this.cache = {};\r\n  }\r\n}\r\n\r\n// Export a singleton instance of the cache\r\nexport const dataCache = new DataCache();\r\n\r\n/**\r\n * Main API service with caching for the Groktutor platform\r\n */\r\nexport const fetchDataWithCache = async (source, forceRefresh = false) => {\r\n  // Define the data fetching functions for each source\r\n  const fetchFunctions = {\r\n    reddit: fetchRedditData,\r\n    twitter: fetchTwitterData,\r\n    github: fetchGithubData,\r\n    huggingface: fetchHuggingfaceData,\r\n    all: fetchAllData\r\n  };\r\n  \r\n  // If force refresh is not required, try to get from cache first\r\n  if (!forceRefresh) {\r\n    const cachedData = dataCache.get(source);\r\n    if (cachedData) {\r\n      return cachedData;\r\n    }\r\n  }\r\n  \r\n  // If not in cache or force refresh, fetch new data\r\n  try {\r\n    const fetchFunction = fetchFunctions[source];\r\n    if (!fetchFunction) {\r\n      throw new Error(`Invalid data source: ${source}`);\r\n    }\r\n    \r\n    const data = await fetchFunction();\r\n    \r\n    // Cache the new data\r\n    dataCache.set(source, data);\r\n    \r\n    return data;\r\n  } catch (error) {\r\n    console.error(`Error fetching ${source} data:`, error);\r\n    throw error;\r\n  }\r\n};\r\n\r\n/**\r\n * Real-time data service for live updates\r\n */\r\nclass RealTimeDataService {\r\n  constructor() {\r\n    this.subscribers = {};\r\n    this.intervalIds = {};\r\n    this.isRunning = false;\r\n  }\r\n  \r\n  start() {\r\n    if (this.isRunning) return;\r\n    this.isRunning = true;\r\n    \r\n    // Set up periodic fetching for each source\r\n    this.intervalIds.reddit = setInterval(async () => {\r\n      try {\r\n        const data = await fetchDataWithCache('reddit', true);\r\n        this.notifySubscribers('reddit', data);\r\n      } catch (error) {\r\n        console.error('Error in real-time Reddit data:', error);\r\n      }\r\n    }, 300000); // 5 minutes\r\n    \r\n    this.intervalIds.twitter = setInterval(async () => {\r\n      try {\r\n        const data = await fetchDataWithCache('twitter', true);\r\n        this.notifySubscribers('twitter', data);\r\n      } catch (error) {\r\n        console.error('Error in real-time Twitter data:', error);\r\n      }\r\n    }, 300000); // 5 minutes\r\n    \r\n    this.intervalIds.github = setInterval(async () => {\r\n      try {\r\n        const data = await fetchDataWithCache('github', true);\r\n        this.notifySubscribers('github', data);\r\n      } catch (error) {\r\n        console.error('Error in real-time GitHub data:', error);\r\n      }\r\n    }, 1800000); // 30 minutes\r\n    \r\n    this.intervalIds.huggingface = setInterval(async () => {\r\n      try {\r\n        const data = await fetchDataWithCache('huggingface', true);\r\n        this.notifySubscribers('huggingface', data);\r\n      } catch (error) {\r\n        console.error('Error in real-time HuggingFace data:', error);\r\n      }\r\n    }, 3600000); // 60 minutes\r\n  }\r\n  \r\n  stop() {\r\n    if (!this.isRunning) return;\r\n    \r\n    // Clear all intervals\r\n    Object.values(this.intervalIds).forEach(intervalId => {\r\n      clearInterval(intervalId);\r\n    });\r\n    \r\n    this.intervalIds = {};\r\n    this.isRunning = false;\r\n  }\r\n  \r\n  subscribe(source, callback) {\r\n    if (!this.subscribers[source]) {\r\n      this.subscribers[source] = [];\r\n    }\r\n    \r\n    this.subscribers[source].push(callback);\r\n    \r\n    // Return an unsubscribe function\r\n    return () => {\r\n      this.subscribers[source] = this.subscribers[source].filter(cb => cb !== callback);\r\n    };\r\n  }\r\n  \r\n  notifySubscribers(source, data) {\r\n    if (this.subscribers[source]) {\r\n      this.subscribers[source].forEach(callback => {\r\n        callback(data);\r\n      });\r\n    }\r\n  }\r\n}\r\n\r\n// Export a singleton instance of the real-time service\r\nexport const realTimeService = new RealTimeDataService();\r\n\r\n// Initialize the real-time service on startup\r\nrealTimeService.start();\r\n"],"mappings":"AAAA;AACA,OAAOA,KAAK,MAAM,OAAO;;AAEzB;AACA,MAAMC,UAAU,GAAG;EACjBC,MAAM,EAAE;IACNC,OAAO,EAAE,0BAA0B;IACnCC,UAAU,EAAE,CAAC,mBAAmB,EAAE,iBAAiB,EAAE,YAAY,EAAE,YAAY,EAAE,QAAQ,EAAE,iBAAiB,CAAC;IAC7GC,QAAQ,EAAEC,OAAO,CAACC,GAAG,CAACC,0BAA0B;IAChDC,YAAY,EAAEH,OAAO,CAACC,GAAG,CAACG;EAC5B,CAAC;EACDC,OAAO,EAAE;IACPR,OAAO,EAAE,2BAA2B;IACpCS,WAAW,EAAEN,OAAO,CAACC,GAAG,CAACM,8BAA8B;IACvDC,aAAa,EAAE,CAAC,oBAAoB,EAAE,eAAe,EAAE,YAAY,EAAE,iBAAiB,EAAE,gBAAgB;EAC1G,CAAC;EACDC,MAAM,EAAE;IACNZ,OAAO,EAAE,wBAAwB;IACjCa,KAAK,EAAEV,OAAO,CAACC,GAAG,CAACU,sBAAsB;IACzCC,MAAM,EAAE,CAAC,oBAAoB,EAAE,KAAK,EAAE,YAAY,EAAE,WAAW,EAAE,aAAa;EAChF,CAAC;EACDC,WAAW,EAAE;IACXhB,OAAO,EAAE,4BAA4B;IACrCa,KAAK,EAAEV,OAAO,CAACC,GAAG,CAACa;EACrB;AACF,CAAC;;AAED;AACA,MAAMC,WAAW,GAAG;EAClBnB,MAAM,EAAE,CACN;IACEoB,EAAE,EAAE,KAAK;IACTC,KAAK,EAAE,8CAA8C;IACrDC,SAAS,EAAE,mBAAmB;IAC9BC,MAAM,EAAE,eAAe;IACvBC,KAAK,EAAE,GAAG;IACVC,YAAY,EAAE,EAAE;IAChBC,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,KAAK;IACTC,KAAK,EAAE,qEAAqE;IAC5EC,SAAS,EAAE,iBAAiB;IAC5BC,MAAM,EAAE,UAAU;IAClBC,KAAK,EAAE,GAAG;IACVC,YAAY,EAAE,EAAE;IAChBC,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,KAAK;IACTC,KAAK,EAAE,yDAAyD;IAChEC,SAAS,EAAE,YAAY;IACvBC,MAAM,EAAE,sBAAsB;IAC9BC,KAAK,EAAE,GAAG;IACVC,YAAY,EAAE,EAAE;IAChBC,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,KAAK;IACTC,KAAK,EAAE,0DAA0D;IACjEC,SAAS,EAAE,YAAY;IACvBC,MAAM,EAAE,sBAAsB;IAC9BC,KAAK,EAAE,GAAG;IACVC,YAAY,EAAE,EAAE;IAChBC,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,KAAK;IACTC,KAAK,EAAE,8DAA8D;IACrEC,SAAS,EAAE,mBAAmB;IAC9BC,MAAM,EAAE,cAAc;IACtBC,KAAK,EAAE,GAAG;IACVC,YAAY,EAAE,EAAE;IAChBC,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEC,QAAQ,EAAE;EACZ,CAAC,CACF;EACDrB,OAAO,EAAE,CACP;IACEW,EAAE,EAAE,KAAK;IACTW,OAAO,EAAE,mMAAmM;IAC5MC,QAAQ,EAAE,gBAAgB;IAC1BC,WAAW,EAAE,8BAA8B;IAC3CC,KAAK,EAAE,GAAG;IACVC,QAAQ,EAAE,EAAE;IACZT,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IAChEO,GAAG,EAAE;EACP,CAAC,EACD;IACEhB,EAAE,EAAE,KAAK;IACTW,OAAO,EAAE,mNAAmN;IAC5NC,QAAQ,EAAE,kBAAkB;IAC5BC,WAAW,EAAE,cAAc;IAC3BC,KAAK,EAAE,GAAG;IACVC,QAAQ,EAAE,GAAG;IACbT,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IAChEO,GAAG,EAAE;EACP,CAAC,EACD;IACEhB,EAAE,EAAE,KAAK;IACTW,OAAO,EAAE,mPAAmP;IAC5PC,QAAQ,EAAE,aAAa;IACvBC,WAAW,EAAE,2BAA2B;IACxCC,KAAK,EAAE,GAAG;IACVC,QAAQ,EAAE,GAAG;IACbT,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEO,GAAG,EAAE;EACP,CAAC,EACD;IACEhB,EAAE,EAAE,KAAK;IACTW,OAAO,EAAE,kQAAkQ;IAC3QC,QAAQ,EAAE,gBAAgB;IAC1BC,WAAW,EAAE,mBAAmB;IAChCC,KAAK,EAAE,GAAG;IACVC,QAAQ,EAAE,EAAE;IACZT,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEO,GAAG,EAAE;EACP,CAAC,EACD;IACEhB,EAAE,EAAE,KAAK;IACTW,OAAO,EAAE,gOAAgO;IACzOC,QAAQ,EAAE,gBAAgB;IAC1BC,WAAW,EAAE,wBAAwB;IACrCC,KAAK,EAAE,GAAG;IACVC,QAAQ,EAAE,GAAG;IACbT,OAAO,EAAE,IAAIC,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACjEO,GAAG,EAAE;EACP,CAAC,CACF;EACDvB,MAAM,EAAE,CACN;IACEO,EAAE,EAAE,KAAK;IACTiB,IAAI,EAAE,0BAA0B;IAChCd,MAAM,EAAE,SAAS;IACjBe,KAAK,EAAE,KAAK;IACZC,KAAK,EAAE,IAAI;IACXC,WAAW,EAAE,sHAAsH;IACnIC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACrEO,GAAG,EAAE,qDAAqD;IAC1DM,QAAQ,EAAE,QAAQ;IAClBC,MAAM,EAAE;EACV,CAAC,EACD;IACEvB,EAAE,EAAE,KAAK;IACTiB,IAAI,EAAE,WAAW;IACjBd,MAAM,EAAE,cAAc;IACtBe,KAAK,EAAE,KAAK;IACZC,KAAK,EAAE,KAAK;IACZC,WAAW,EAAE,uDAAuD;IACpEC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACrEO,GAAG,EAAE,2CAA2C;IAChDM,QAAQ,EAAE,QAAQ;IAClBC,MAAM,EAAE;EACV,CAAC,EACD;IACEvB,EAAE,EAAE,KAAK;IACTiB,IAAI,EAAE,gBAAgB;IACtBd,MAAM,EAAE,SAAS;IACjBe,KAAK,EAAE,IAAI;IACXC,KAAK,EAAE,IAAI;IACXC,WAAW,EAAE,4GAA4G;IACzHC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACrEO,GAAG,EAAE,2CAA2C;IAChDM,QAAQ,EAAE,YAAY;IACtBC,MAAM,EAAE;EACV,CAAC,EACD;IACEvB,EAAE,EAAE,KAAK;IACTiB,IAAI,EAAE,4BAA4B;IAClCd,MAAM,EAAE,YAAY;IACpBe,KAAK,EAAE,KAAK;IACZC,KAAK,EAAE,IAAI;IACXC,WAAW,EAAE,sFAAsF;IACnGC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACrEO,GAAG,EAAE,0DAA0D;IAC/DM,QAAQ,EAAE,UAAU;IACpBC,MAAM,EAAE;EACV,CAAC,EACD;IACEvB,EAAE,EAAE,KAAK;IACTiB,IAAI,EAAE,qBAAqB;IAC3Bd,MAAM,EAAE,UAAU;IAClBe,KAAK,EAAE,IAAI;IACXC,KAAK,EAAE,GAAG;IACVC,WAAW,EAAE,wFAAwF;IACrGC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACrEO,GAAG,EAAE,iDAAiD;IACtDM,QAAQ,EAAE,QAAQ;IAClBC,MAAM,EAAE;EACV,CAAC,CACF;EACD1B,WAAW,EAAE,CACX;IACEG,EAAE,EAAE,KAAK;IACTwB,KAAK,EAAE,uBAAuB;IAC9BrB,MAAM,EAAE,mBAAmB;IAC3BsB,SAAS,EAAE,KAAK;IAChBL,WAAW,EAAE,6DAA6D;IAC1EC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACtEO,GAAG,EAAE,gEAAgE;IACrEU,IAAI,EAAE,CAAC,eAAe,EAAE,KAAK,EAAE,cAAc;EAC/C,CAAC,EACD;IACE1B,EAAE,EAAE,KAAK;IACTwB,KAAK,EAAE,uBAAuB;IAC9BrB,MAAM,EAAE,aAAa;IACrBsB,SAAS,EAAE,IAAI;IACfL,WAAW,EAAE,uEAAuE;IACpFC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACtEO,GAAG,EAAE,0DAA0D;IAC/DU,IAAI,EAAE,CAAC,OAAO,EAAE,WAAW,EAAE,KAAK;EACpC,CAAC,EACD;IACE1B,EAAE,EAAE,KAAK;IACTwB,KAAK,EAAE,wBAAwB;IAC/BrB,MAAM,EAAE,YAAY;IACpBsB,SAAS,EAAE,IAAI;IACfL,WAAW,EAAE,uDAAuD;IACpEC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACtEO,GAAG,EAAE,0DAA0D;IAC/DU,IAAI,EAAE,CAAC,kBAAkB,EAAE,OAAO,EAAE,WAAW;EACjD,CAAC,EACD;IACE1B,EAAE,EAAE,KAAK;IACTwB,KAAK,EAAE,4BAA4B;IACnCrB,MAAM,EAAE,kBAAkB;IAC1BsB,SAAS,EAAE,KAAK;IAChBL,WAAW,EAAE,kEAAkE;IAC/EC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACrEO,GAAG,EAAE,6EAA6E;IAClFU,IAAI,EAAE,CAAC,SAAS,EAAE,oBAAoB,EAAE,UAAU;EACpD,CAAC,EACD;IACE1B,EAAE,EAAE,KAAK;IACTwB,KAAK,EAAE,uBAAuB;IAC9BrB,MAAM,EAAE,eAAe;IACvBsB,SAAS,EAAE,IAAI;IACfL,WAAW,EAAE,0EAA0E;IACvFC,OAAO,EAAE,IAAId,IAAI,CAACA,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,IAAI,CAAC,CAACC,WAAW,CAAC,CAAC;IACtEO,GAAG,EAAE,4DAA4D;IACjEU,IAAI,EAAE,CAAC,YAAY,EAAE,SAAS,EAAE,eAAe;EACjD,CAAC;AAEL,CAAC;;AAED;AACA;AACA;AACA;AACA,OAAO,MAAMC,eAAe,GAAG,MAAAA,CAAOC,KAAK,GAAG,EAAE,KAAK;EACnD,IAAI;IACF;IACA,IAAI5C,OAAO,CAACC,GAAG,CAAC4C,QAAQ,KAAK,aAAa,IAAI,CAAClD,UAAU,CAACC,MAAM,CAACG,QAAQ,EAAE;MACzE+C,OAAO,CAACC,GAAG,CAAC,0BAA0B,CAAC;MACvC,OAAOhC,WAAW,CAACnB,MAAM,CAACoD,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;IAC3C;;IAEA;IACA;IACA,MAAMK,aAAa,GAAG,MAAMvD,KAAK,CAACwD,IAAI,CAAC,4CAA4C,EACjF,+BAA+B,EAC/B;MACEC,OAAO,EAAE;QACP,cAAc,EAAE,mCAAmC;QACnD,eAAe,EAAE,SAASC,IAAI,CAAC,GAAGzD,UAAU,CAACC,MAAM,CAACG,QAAQ,IAAIJ,UAAU,CAACC,MAAM,CAACO,YAAY,EAAE,CAAC;MACnG;IACF,CACF,CAAC;IAED,MAAMkD,WAAW,GAAGJ,aAAa,CAACK,IAAI,CAACC,YAAY;;IAEnD;IACA,MAAMC,gBAAgB,GAAG7D,UAAU,CAACC,MAAM,CAACE,UAAU,CAAC2D,GAAG,CAACC,GAAG,IAC3DhE,KAAK,CAACiE,GAAG,CAAC,GAAGhE,UAAU,CAACC,MAAM,CAACC,OAAO,MAAM6D,GAAG,cAAcE,IAAI,CAACC,IAAI,CAACjB,KAAK,GAAGjD,UAAU,CAACC,MAAM,CAACE,UAAU,CAACgE,MAAM,CAAC,EAAE,EAAE;MACrHX,OAAO,EAAE;QACP,eAAe,EAAE,UAAUE,WAAW,EAAE;QACxC,YAAY,EAAE;MAChB;IACF,CAAC,CACH,CAAC;;IAED;IACA,MAAMU,SAAS,GAAG,MAAMC,OAAO,CAACC,GAAG,CAACT,gBAAgB,CAAC;;IAErD;IACA,IAAIU,QAAQ,GAAG,EAAE;IACjBH,SAAS,CAACI,OAAO,CAACC,QAAQ,IAAI;MAC5B,IAAIA,QAAQ,CAACd,IAAI,CAACA,IAAI,IAAIc,QAAQ,CAACd,IAAI,CAACA,IAAI,CAACe,QAAQ,EAAE;QACrD,MAAMC,cAAc,GAAGF,QAAQ,CAACd,IAAI,CAACA,IAAI,CAACe,QAAQ,CAACZ,GAAG,CAACc,KAAK,KAAK;UAC/DvD,EAAE,EAAEuD,KAAK,CAACjB,IAAI,CAACtC,EAAE;UACjBC,KAAK,EAAEsD,KAAK,CAACjB,IAAI,CAACrC,KAAK;UACvBG,KAAK,EAAEmD,KAAK,CAACjB,IAAI,CAAClC,KAAK;UACvBD,MAAM,EAAEoD,KAAK,CAACjB,IAAI,CAACnC,MAAM;UACzBD,SAAS,EAAE,KAAKqD,KAAK,CAACjB,IAAI,CAACpC,SAAS,EAAE;UACtCI,OAAO,EAAE,IAAIC,IAAI,CAACgD,KAAK,CAACjB,IAAI,CAACkB,WAAW,GAAG,IAAI,CAAC,CAAC/C,WAAW,CAAC,CAAC;UAC9DO,GAAG,EAAE,qBAAqBuC,KAAK,CAACjB,IAAI,CAACmB,SAAS,EAAE;UAChDpD,YAAY,EAAEkD,KAAK,CAACjB,IAAI,CAACjC,YAAY;UACrCK,QAAQ,EAAE6C,KAAK,CAACjB,IAAI,CAAC5B;QACvB,CAAC,CAAC,CAAC;QACHwC,QAAQ,GAAG,CAAC,GAAGA,QAAQ,EAAE,GAAGI,cAAc,CAAC;MAC7C;IACF,CAAC,CAAC;;IAEF;IACA,OAAOJ,QAAQ,CAACQ,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAACxD,KAAK,GAAGuD,CAAC,CAACvD,KAAK,CAAC,CAAC4B,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EACnE,CAAC,CAAC,OAAOiC,KAAK,EAAE;IACd/B,OAAO,CAAC+B,KAAK,CAAC,6BAA6B,EAAEA,KAAK,CAAC;IACnD;IACA,OAAO9D,WAAW,CAACnB,MAAM,CAACoD,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EAC3C;AACF,CAAC;;AAED;AACA;AACA;AACA;AACA,OAAO,MAAMkC,gBAAgB,GAAG,MAAAA,CAAOlC,KAAK,GAAG,EAAE,KAAK;EACpD,IAAI;IACF;IACA,IAAI5C,OAAO,CAACC,GAAG,CAAC4C,QAAQ,KAAK,aAAa,IAAI,CAAClD,UAAU,CAACU,OAAO,CAACC,WAAW,EAAE;MAC7EwC,OAAO,CAACC,GAAG,CAAC,2BAA2B,CAAC;MACxC,OAAOhC,WAAW,CAACV,OAAO,CAAC2C,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;IAC5C;;IAEA;IACA;IACA,MAAMpC,aAAa,GAAGb,UAAU,CAACU,OAAO,CAACG,aAAa,CAACiD,GAAG,CAACsB,KAAK,IAC9DrF,KAAK,CAACiE,GAAG,CAAC,GAAGhE,UAAU,CAACU,OAAO,CAACR,OAAO,+BAA+BmF,kBAAkB,CAACD,KAAK,CAAC,gBAAgBnB,IAAI,CAACC,IAAI,CAACjB,KAAK,GAAGjD,UAAU,CAACU,OAAO,CAACG,aAAa,CAACsD,MAAM,CAAC,wFAAwF,EAAE;MACjQX,OAAO,EAAE;QACP,eAAe,EAAE,UAAUxD,UAAU,CAACU,OAAO,CAACC,WAAW;MAC3D;IACF,CAAC,CACH,CAAC;;IAED;IACA,MAAMyD,SAAS,GAAG,MAAMC,OAAO,CAACC,GAAG,CAACzD,aAAa,CAAC;;IAElD;IACA,IAAIyE,SAAS,GAAG,EAAE;IAClBlB,SAAS,CAACI,OAAO,CAACC,QAAQ,IAAI;MAC5B,IAAIA,QAAQ,CAACd,IAAI,IAAIc,QAAQ,CAACd,IAAI,CAACA,IAAI,EAAE;QACvC;QACA,MAAM4B,KAAK,GAAG,CAAC,CAAC;QAChB,IAAId,QAAQ,CAACd,IAAI,CAAC6B,QAAQ,IAAIf,QAAQ,CAACd,IAAI,CAAC6B,QAAQ,CAACD,KAAK,EAAE;UAC1Dd,QAAQ,CAACd,IAAI,CAAC6B,QAAQ,CAACD,KAAK,CAACf,OAAO,CAACiB,IAAI,IAAI;YAC3CF,KAAK,CAACE,IAAI,CAACpE,EAAE,CAAC,GAAG;cACfY,QAAQ,EAAEwD,IAAI,CAACxD,QAAQ;cACvBC,WAAW,EAAEuD,IAAI,CAACC;YACpB,CAAC;UACH,CAAC,CAAC;QACJ;QAEA,MAAMC,MAAM,GAAGlB,QAAQ,CAACd,IAAI,CAACA,IAAI,CAACG,GAAG,CAAC8B,KAAK;UAAA,IAAAC,qBAAA;UAAA,OAAK;YAC9CxE,EAAE,EAAEuE,KAAK,CAACvE,EAAE;YACZW,OAAO,EAAE4D,KAAK,CAACE,IAAI;YACnB7D,QAAQ,EAAEsD,KAAK,CAACK,KAAK,CAACG,SAAS,CAAC,GAAG,IAAIR,KAAK,CAACK,KAAK,CAACG,SAAS,CAAC,CAAC9D,QAAQ,EAAE,GAAG,UAAU;YACrFC,WAAW,EAAEqD,KAAK,CAACK,KAAK,CAACG,SAAS,CAAC,GAAGR,KAAK,CAACK,KAAK,CAACG,SAAS,CAAC,CAAC7D,WAAW,GAAG,cAAc;YACzFC,KAAK,EAAEyD,KAAK,CAACI,cAAc,CAACC,UAAU;YACtC7D,QAAQ,EAAEwD,KAAK,CAACI,cAAc,CAACE,aAAa;YAC5CvE,OAAO,EAAEiE,KAAK,CAACO,UAAU;YACzB9D,GAAG,EAAE,wBAAAwD,qBAAA,GAAuBN,KAAK,CAACK,KAAK,CAACG,SAAS,CAAC,cAAAF,qBAAA,uBAAtBA,qBAAA,CAAwB5D,QAAQ,WAAW2D,KAAK,CAACvE,EAAE;UACjF,CAAC;QAAA,CAAC,CAAC;QAEHiE,SAAS,GAAG,CAAC,GAAGA,SAAS,EAAE,GAAGK,MAAM,CAAC;MACvC;IACF,CAAC,CAAC;;IAEF;IACA,OAAOL,SAAS,CACbP,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAMA,CAAC,CAAC9C,KAAK,GAAG8C,CAAC,CAAC7C,QAAQ,IAAK4C,CAAC,CAAC7C,KAAK,GAAG6C,CAAC,CAAC5C,QAAQ,CAAC,CAAC,CAC/DiB,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EACpB,CAAC,CAAC,OAAOiC,KAAK,EAAE;IACd/B,OAAO,CAAC+B,KAAK,CAAC,8BAA8B,EAAEA,KAAK,CAAC;IACpD;IACA,OAAO9D,WAAW,CAACV,OAAO,CAAC2C,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EAC5C;AACF,CAAC;;AAED;AACA;AACA;AACA;AACA,OAAO,MAAMmD,eAAe,GAAG,MAAAA,CAAOnD,KAAK,GAAG,EAAE,KAAK;EACnD,IAAI;IACF;IACA,IAAI5C,OAAO,CAACC,GAAG,CAAC4C,QAAQ,KAAK,aAAa,IAAI,CAAClD,UAAU,CAACc,MAAM,CAACC,KAAK,EAAE;MACtEoC,OAAO,CAACC,GAAG,CAAC,0BAA0B,CAAC;MACvC,OAAOhC,WAAW,CAACN,MAAM,CAACuC,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;IAC3C;;IAEA;IACA;IACA,MAAMoD,YAAY,GAAGrG,UAAU,CAACc,MAAM,CAACG,MAAM,CAAC6C,GAAG,CAACwC,KAAK,IACrDvG,KAAK,CAACiE,GAAG,CAAC,GAAGhE,UAAU,CAACc,MAAM,CAACZ,OAAO,gCAAgCoG,KAAK,mCAAmCrC,IAAI,CAACC,IAAI,CAACjB,KAAK,GAAGjD,UAAU,CAACc,MAAM,CAACG,MAAM,CAACkD,MAAM,CAAC,EAAE,EAAE;MAClKX,OAAO,EAAE;QACP,eAAe,EAAE,SAASxD,UAAU,CAACc,MAAM,CAACC,KAAK,EAAE;QACnD,QAAQ,EAAE;MACZ;IACF,CAAC,CACH,CAAC;;IAED;IACA,MAAMqD,SAAS,GAAG,MAAMC,OAAO,CAACC,GAAG,CAAC+B,YAAY,CAAC;;IAEjD;IACA,IAAIE,QAAQ,GAAG,EAAE;IACjBnC,SAAS,CAACI,OAAO,CAACC,QAAQ,IAAI;MAC5B,IAAIA,QAAQ,CAACd,IAAI,IAAIc,QAAQ,CAACd,IAAI,CAAC6C,KAAK,EAAE;QACxC,MAAMC,KAAK,GAAGhC,QAAQ,CAACd,IAAI,CAAC6C,KAAK,CAAC1C,GAAG,CAAC4C,IAAI,KAAK;UAC7CrF,EAAE,EAAEqF,IAAI,CAACrF,EAAE,CAACsF,QAAQ,CAAC,CAAC;UACtBrE,IAAI,EAAEoE,IAAI,CAAChB,IAAI;UACflE,MAAM,EAAEkF,IAAI,CAACE,KAAK,CAACC,KAAK;UACxBtE,KAAK,EAAEmE,IAAI,CAACI,gBAAgB;UAC5BrE,WAAW,EAAEiE,IAAI,CAACjE,WAAW,IAAI,EAAE;UACnCC,OAAO,EAAEgE,IAAI,CAACK,UAAU;UACxB1E,GAAG,EAAEqE,IAAI,CAACM,QAAQ;UAClBrE,QAAQ,EAAE+D,IAAI,CAAC/D,QAAQ;UACvBH,KAAK,EAAEkE,IAAI,CAACO,WAAW;UACvBrE,MAAM,EAAE8D,IAAI,CAACQ;QACf,CAAC,CAAC,CAAC;QAEHX,QAAQ,GAAG,CAAC,GAAGA,QAAQ,EAAE,GAAGE,KAAK,CAAC;MACpC;IACF,CAAC,CAAC;;IAEF;IACA,MAAMU,WAAW,GAAGZ,QAAQ,CAACa,MAAM,CAAC,CAACC,GAAG,EAAEC,OAAO,KAAK;MACpD,MAAMC,CAAC,GAAGF,GAAG,CAACG,IAAI,CAACd,IAAI,IAAIA,IAAI,CAACrF,EAAE,KAAKiG,OAAO,CAACjG,EAAE,CAAC;MAClD,IAAI,CAACkG,CAAC,EAAE;QACN,OAAOF,GAAG,CAACI,MAAM,CAAC,CAACH,OAAO,CAAC,CAAC;MAC9B,CAAC,MAAM;QACL,OAAOD,GAAG;MACZ;IACF,CAAC,EAAE,EAAE,CAAC;;IAEN;IACA,OAAOF,WAAW,CAACpC,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAAC1C,KAAK,GAAGyC,CAAC,CAACzC,KAAK,CAAC,CAACc,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EACtE,CAAC,CAAC,OAAOiC,KAAK,EAAE;IACd/B,OAAO,CAAC+B,KAAK,CAAC,6BAA6B,EAAEA,KAAK,CAAC;IACnD;IACA,OAAO9D,WAAW,CAACN,MAAM,CAACuC,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EAC3C;AACF,CAAC;;AAED;AACA;AACA;AACA;AACA,OAAO,MAAMyE,oBAAoB,GAAG,MAAAA,CAAOzE,KAAK,GAAG,EAAE,KAAK;EACxD,IAAI;IACF;IACA,IAAI5C,OAAO,CAACC,GAAG,CAAC4C,QAAQ,KAAK,aAAa,IAAI,CAAClD,UAAU,CAACkB,WAAW,CAACH,KAAK,EAAE;MAC3EoC,OAAO,CAACC,GAAG,CAAC,+BAA+B,CAAC;MAC5C,OAAOhC,WAAW,CAACF,WAAW,CAACmC,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;IAChD;;IAEA;IACA;IACA,MAAMwB,QAAQ,GAAG,MAAM1E,KAAK,CAACiE,GAAG,CAAC,GAAGhE,UAAU,CAACkB,WAAW,CAAChB,OAAO,2DAA2D+C,KAAK,EAAE,EAAE;MACpIO,OAAO,EAAE;QACP,eAAe,EAAE,UAAUxD,UAAU,CAACkB,WAAW,CAACH,KAAK;MACzD;IACF,CAAC,CAAC;;IAEF;IACA,MAAM4G,MAAM,GAAGlD,QAAQ,CAACd,IAAI,CAACG,GAAG,CAACjB,KAAK,KAAK;MACzCxB,EAAE,EAAEwB,KAAK,CAACxB,EAAE;MACZwB,KAAK,EAAEA,KAAK,CAACxB,EAAE,CAACuG,KAAK,CAAC,GAAG,CAAC,CAACC,GAAG,CAAC,CAAC;MAChCrG,MAAM,EAAEqB,KAAK,CAACxB,EAAE,CAACuG,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;MAC9B9E,SAAS,EAAED,KAAK,CAACC,SAAS,IAAI,CAAC;MAC/BL,WAAW,EAAEI,KAAK,CAACJ,WAAW,IAAI,0BAA0B;MAC5DC,OAAO,EAAEG,KAAK,CAACiF,YAAY;MAC3BzF,GAAG,EAAE,0BAA0BQ,KAAK,CAACxB,EAAE,EAAE;MACzC0B,IAAI,EAAEF,KAAK,CAACE,IAAI,IAAI;IACtB,CAAC,CAAC,CAAC;IAEH,OAAO4E,MAAM;EACf,CAAC,CAAC,OAAOzC,KAAK,EAAE;IACd/B,OAAO,CAAC+B,KAAK,CAAC,kCAAkC,EAAEA,KAAK,CAAC;IACxD;IACA,OAAO9D,WAAW,CAACF,WAAW,CAACmC,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAAC;EAChD;AACF,CAAC;;AAED;AACA;AACA;AACA,OAAO,MAAM8E,YAAY,GAAG,MAAAA,CAAA,KAAY;EACtC,IAAI;IACF,MAAM,CAACC,UAAU,EAAEC,WAAW,EAAEC,UAAU,EAAEC,eAAe,CAAC,GAAG,MAAM9D,OAAO,CAACC,GAAG,CAAC,CAC/EtB,eAAe,CAAC,CAAC,EACjBmC,gBAAgB,CAAC,CAAC,EAClBiB,eAAe,CAAC,CAAC,EACjBsB,oBAAoB,CAAC,CAAC,CACvB,CAAC;IAEF,OAAO;MACLzH,MAAM,EAAE+H,UAAU;MAClBtH,OAAO,EAAEuH,WAAW;MACpBnH,MAAM,EAAEoH,UAAU;MAClBhH,WAAW,EAAEiH,eAAe;MAC5BC,SAAS,EAAE,IAAIxG,IAAI,CAAC,CAAC,CAACE,WAAW,CAAC;IACpC,CAAC;EACH,CAAC,CAAC,OAAOoD,KAAK,EAAE;IACd/B,OAAO,CAAC+B,KAAK,CAAC,0BAA0B,EAAEA,KAAK,CAAC;IAChD,OAAO;MACLjF,MAAM,EAAEmB,WAAW,CAACnB,MAAM;MAC1BS,OAAO,EAAEU,WAAW,CAACV,OAAO;MAC5BI,MAAM,EAAEM,WAAW,CAACN,MAAM;MAC1BI,WAAW,EAAEE,WAAW,CAACF,WAAW;MACpCkH,SAAS,EAAE,IAAIxG,IAAI,CAAC,CAAC,CAACE,WAAW,CAAC,CAAC;MACnCoD,KAAK,EAAEA,KAAK,CAACmD;IACf,CAAC;EACH;AACF,CAAC;;AAED;AACA;AACA;AACA,MAAMC,SAAS,CAAC;EACdC,WAAWA,CAACC,aAAa,GAAG,MAAM,EAAE;IAAE;IACpC,IAAI,CAACC,KAAK,GAAG,CAAC,CAAC;IACf,IAAI,CAACD,aAAa,GAAGA,aAAa;EACpC;EAEAxE,GAAGA,CAAC0E,GAAG,EAAE;IACP,MAAMC,UAAU,GAAG,IAAI,CAACF,KAAK,CAACC,GAAG,CAAC;IAClC,IAAIC,UAAU,IAAK/G,IAAI,CAACC,GAAG,CAAC,CAAC,GAAG8G,UAAU,CAACP,SAAS,GAAG,IAAI,CAACI,aAAc,EAAE;MAC1E,OAAOG,UAAU,CAAChF,IAAI;IACxB;IACA,OAAO,IAAI;EACb;EAEAiF,GAAGA,CAACF,GAAG,EAAE/E,IAAI,EAAE;IACb,IAAI,CAAC8E,KAAK,CAACC,GAAG,CAAC,GAAG;MAChB/E,IAAI;MACJyE,SAAS,EAAExG,IAAI,CAACC,GAAG,CAAC;IACtB,CAAC;EACH;EAEAgH,UAAUA,CAACH,GAAG,EAAE;IACd,OAAO,IAAI,CAACD,KAAK,CAACC,GAAG,CAAC;EACxB;EAEAI,aAAaA,CAAA,EAAG;IACd,IAAI,CAACL,KAAK,GAAG,CAAC,CAAC;EACjB;AACF;;AAEA;AACA,OAAO,MAAMM,SAAS,GAAG,IAAIT,SAAS,CAAC,CAAC;;AAExC;AACA;AACA;AACA,OAAO,MAAMU,kBAAkB,GAAG,MAAAA,CAAOC,MAAM,EAAEC,YAAY,GAAG,KAAK,KAAK;EACxE;EACA,MAAMC,cAAc,GAAG;IACrBlJ,MAAM,EAAE+C,eAAe;IACvBtC,OAAO,EAAEyE,gBAAgB;IACzBrE,MAAM,EAAEsF,eAAe;IACvBlF,WAAW,EAAEwG,oBAAoB;IACjCpD,GAAG,EAAEyD;EACP,CAAC;;EAED;EACA,IAAI,CAACmB,YAAY,EAAE;IACjB,MAAMP,UAAU,GAAGI,SAAS,CAAC/E,GAAG,CAACiF,MAAM,CAAC;IACxC,IAAIN,UAAU,EAAE;MACd,OAAOA,UAAU;IACnB;EACF;;EAEA;EACA,IAAI;IACF,MAAMS,aAAa,GAAGD,cAAc,CAACF,MAAM,CAAC;IAC5C,IAAI,CAACG,aAAa,EAAE;MAClB,MAAM,IAAIC,KAAK,CAAC,wBAAwBJ,MAAM,EAAE,CAAC;IACnD;IAEA,MAAMtF,IAAI,GAAG,MAAMyF,aAAa,CAAC,CAAC;;IAElC;IACAL,SAAS,CAACH,GAAG,CAACK,MAAM,EAAEtF,IAAI,CAAC;IAE3B,OAAOA,IAAI;EACb,CAAC,CAAC,OAAOuB,KAAK,EAAE;IACd/B,OAAO,CAAC+B,KAAK,CAAC,kBAAkB+D,MAAM,QAAQ,EAAE/D,KAAK,CAAC;IACtD,MAAMA,KAAK;EACb;AACF,CAAC;;AAED;AACA;AACA;AACA,MAAMoE,mBAAmB,CAAC;EACxBf,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACgB,WAAW,GAAG,CAAC,CAAC;IACrB,IAAI,CAACC,WAAW,GAAG,CAAC,CAAC;IACrB,IAAI,CAACC,SAAS,GAAG,KAAK;EACxB;EAEAC,KAAKA,CAAA,EAAG;IACN,IAAI,IAAI,CAACD,SAAS,EAAE;IACpB,IAAI,CAACA,SAAS,GAAG,IAAI;;IAErB;IACA,IAAI,CAACD,WAAW,CAACvJ,MAAM,GAAG0J,WAAW,CAAC,YAAY;MAChD,IAAI;QACF,MAAMhG,IAAI,GAAG,MAAMqF,kBAAkB,CAAC,QAAQ,EAAE,IAAI,CAAC;QACrD,IAAI,CAACY,iBAAiB,CAAC,QAAQ,EAAEjG,IAAI,CAAC;MACxC,CAAC,CAAC,OAAOuB,KAAK,EAAE;QACd/B,OAAO,CAAC+B,KAAK,CAAC,iCAAiC,EAAEA,KAAK,CAAC;MACzD;IACF,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC;;IAEZ,IAAI,CAACsE,WAAW,CAAC9I,OAAO,GAAGiJ,WAAW,CAAC,YAAY;MACjD,IAAI;QACF,MAAMhG,IAAI,GAAG,MAAMqF,kBAAkB,CAAC,SAAS,EAAE,IAAI,CAAC;QACtD,IAAI,CAACY,iBAAiB,CAAC,SAAS,EAAEjG,IAAI,CAAC;MACzC,CAAC,CAAC,OAAOuB,KAAK,EAAE;QACd/B,OAAO,CAAC+B,KAAK,CAAC,kCAAkC,EAAEA,KAAK,CAAC;MAC1D;IACF,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC;;IAEZ,IAAI,CAACsE,WAAW,CAAC1I,MAAM,GAAG6I,WAAW,CAAC,YAAY;MAChD,IAAI;QACF,MAAMhG,IAAI,GAAG,MAAMqF,kBAAkB,CAAC,QAAQ,EAAE,IAAI,CAAC;QACrD,IAAI,CAACY,iBAAiB,CAAC,QAAQ,EAAEjG,IAAI,CAAC;MACxC,CAAC,CAAC,OAAOuB,KAAK,EAAE;QACd/B,OAAO,CAAC+B,KAAK,CAAC,iCAAiC,EAAEA,KAAK,CAAC;MACzD;IACF,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC;;IAEb,IAAI,CAACsE,WAAW,CAACtI,WAAW,GAAGyI,WAAW,CAAC,YAAY;MACrD,IAAI;QACF,MAAMhG,IAAI,GAAG,MAAMqF,kBAAkB,CAAC,aAAa,EAAE,IAAI,CAAC;QAC1D,IAAI,CAACY,iBAAiB,CAAC,aAAa,EAAEjG,IAAI,CAAC;MAC7C,CAAC,CAAC,OAAOuB,KAAK,EAAE;QACd/B,OAAO,CAAC+B,KAAK,CAAC,sCAAsC,EAAEA,KAAK,CAAC;MAC9D;IACF,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC;EACf;EAEA2E,IAAIA,CAAA,EAAG;IACL,IAAI,CAAC,IAAI,CAACJ,SAAS,EAAE;;IAErB;IACAK,MAAM,CAACC,MAAM,CAAC,IAAI,CAACP,WAAW,CAAC,CAAChF,OAAO,CAACwF,UAAU,IAAI;MACpDC,aAAa,CAACD,UAAU,CAAC;IAC3B,CAAC,CAAC;IAEF,IAAI,CAACR,WAAW,GAAG,CAAC,CAAC;IACrB,IAAI,CAACC,SAAS,GAAG,KAAK;EACxB;EAEAS,SAASA,CAACjB,MAAM,EAAEkB,QAAQ,EAAE;IAC1B,IAAI,CAAC,IAAI,CAACZ,WAAW,CAACN,MAAM,CAAC,EAAE;MAC7B,IAAI,CAACM,WAAW,CAACN,MAAM,CAAC,GAAG,EAAE;IAC/B;IAEA,IAAI,CAACM,WAAW,CAACN,MAAM,CAAC,CAACmB,IAAI,CAACD,QAAQ,CAAC;;IAEvC;IACA,OAAO,MAAM;MACX,IAAI,CAACZ,WAAW,CAACN,MAAM,CAAC,GAAG,IAAI,CAACM,WAAW,CAACN,MAAM,CAAC,CAACoB,MAAM,CAACC,EAAE,IAAIA,EAAE,KAAKH,QAAQ,CAAC;IACnF,CAAC;EACH;EAEAP,iBAAiBA,CAACX,MAAM,EAAEtF,IAAI,EAAE;IAC9B,IAAI,IAAI,CAAC4F,WAAW,CAACN,MAAM,CAAC,EAAE;MAC5B,IAAI,CAACM,WAAW,CAACN,MAAM,CAAC,CAACzE,OAAO,CAAC2F,QAAQ,IAAI;QAC3CA,QAAQ,CAACxG,IAAI,CAAC;MAChB,CAAC,CAAC;IACJ;EACF;AACF;;AAEA;AACA,OAAO,MAAM4G,eAAe,GAAG,IAAIjB,mBAAmB,CAAC,CAAC;;AAExD;AACAiB,eAAe,CAACb,KAAK,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}